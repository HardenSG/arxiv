# 2602.22070: 语言模型对算法Agent和人类专家存在不一致偏见的深度分析

## 1. 论文基本信息
- **标题**: Language Models Exhibit Inconsistent Biases Towards Algorithmic Agents and Human Experts
- **作者 / 机构**: Jessica Bo 等（未提供具体机构信息）
- **提交时间**: 2026年2月25日 (v1)，投给IASEAI 2026安全与伦理人工智能会议
- **研究领域**: 人工智能 (cs.AI)，大语言模型，AI安全与伦理，人机交互
- **核心问题**: 大语言模型越来越多地被用于决策任务，需要处理来自不同来源的信息，包括人类专家和算法Agent。目前我们不知道LLM会如何权衡这两类不同来源的信息，是否存在系统性偏见，这种偏见是否一致。
- **该问题的重要性**: 
  当前LLM越来越多地作为决策协调者，整合人类专家和AI算法输出的信息，做出最终决策。如果LLM对不同来源存在不一致的系统性偏见，可能会导致决策偏差，在高风险场景（医疗、法律、金融）下会带来严重后果。之前的研究只关注人类对算法的厌恶偏见（algorithm aversion），但从未研究过LLM本身对算法Agent和人类专家的偏见，这个研究填补了重要空白，对LLM安全部署非常关键。

---

## 2. 研究背景与动机
### 现有工作不足
1. **算法厌恶研究只针对人类**: 行为经济学领域早就发现人类存在算法厌恶，即人类决策者倾向于不信任算法预测，哪怕算法预测比人类更准。但现有研究都聚焦在人类身上，没有人研究当LLM作为决策者时，是否也会对算法Agent存在类似偏见，或者相反的偏见。
2. **LLM决策偏见研究不完整**: 现有LLM偏见研究大多集中在性别、种族、宗教等社会偏见，很少研究LLM对不同信息来源（人类vs算法）的偏见，而这种来源偏见对实际决策影响非常大。
3. **任务呈现方式影响偏见**: 之前很少研究任务呈现方式（陈述偏好vs显示偏好）如何影响LLM的偏见表达，这对LLM评估鲁棒性非常重要。

### 论文识别的研究空白
当LLM需要整合人类专家和算法Agent两类不同来源的信息时，LLM会如何权衡？是否存在偏见？偏见是否会因为任务呈现方式不同而不一致？这个核心问题从未被系统性研究过，本文填补了这个空白。

### 与已有LLM工作的关系与差异
现有很多LLM agent研究关注如何让LLM调用其他工具（包括其他AI算法），但都默认LLM会合理使用工具输出，没有考虑LLM本身可能对算法工具存在系统性偏见，而且这种偏见还不一致。本文揭示了LLM工具使用中一个之前被忽略的风险，对设计更可靠的LLM agent系统有重要启示。

### 关键假设
1. LLM能够理解问题描述中"人类专家"和"算法agent"的身份框架；
2. LLM的回答能够反映其内在的偏见倾向；
3. 实验设计能够区分陈述偏好和显示偏好两种不同的偏见表达。

---

## 3. 核心贡献（明确分类）
### 理论贡献
1. 首次系统性研究了大语言模型对算法Agent和人类专家的来源偏见，发现了**不一致偏见**这一重要现象；
2. 揭示了任务呈现方式（陈述偏好vs显示偏好）会导致LLM表达完全相反的偏见，提醒LLM评估领域要重视评估鲁棒性问题。

### 方法/算法贡献
1. 借鉴行为经济学的实验范式，设计了两种不同任务呈现方式评估LLM来源偏见：陈述偏好（直接询问信任度）和显示偏好（给出绩效例子后让LLM下注）；
2. 在8个不同LLM上做了对比实验，结论具有普遍性。

### 实验/经验贡献
1. 在陈述偏好实验中：LLM和人类类似，更信任人类专家，给人类专家更高信任评分，表现出类似人类的算法厌恶；
2. 在显示偏好实验中：当给出绩效例子让LLM下注时，LLM反而 disproportionately 选择算法，哪怕算法表现明显更差，表现出相反的算法偏好；
3. 证实LLM对算法Agent和人类专家存在**不一致偏见**，在不同任务呈现方式下偏见方向完全相反；
4. 这个结论在测试的8个不同LLM上都一致成立，说明这是一个普遍现象，不依赖特定模型。

---

## 4. 方法论深度解析（技术核心）
### 实验设计
本文采用经典的行为实验方法，设计了两个实验条件：
1. **陈述偏好（Stated Preferences）**: 直接询问LLM，在不同任务场景下，你更信任人类专家还是算法Agent，让LLM给两者信任度打分（1-10分）；
2. **显示偏好（Revealed Preferences）**: 给LLM展示人类专家和算法Agent在某个任务上的实际绩效表现（比如算法犯错几次，人类犯错几次），然后让LLm把"赌注"押在它认为更准的一方，通过实际选择揭示偏好。

实验覆盖了多个不同任务场景，从医疗诊断到金融预测再到天气预报，一共8个不同LLM模型参与测试，保证结论的普遍性。

### 核心研究问题
本文要回答两个核心问题：
1. LLM是否像人类一样，对算法Agent存在算法厌恶偏见？
2. 偏见表达是否会因为任务呈现方式不同而发生变化？

### 关键结果的形式化理解
我们可以把实验结果总结为：
- 在陈述偏好条件：
  $$E[Trust(human)] > E[Trust(algorithm)]$$
  和人类算法厌恶一致。
- 在显示偏好条件：
  $$P(Choose\ algorithm | performance\ equal) > P(Choose\ human)$$
  即使算法绩效更差，LLM还是更大概率选算法，完全反过来。

这种不一致性是本文最核心的发现，非常出人意料。

### 为什么会出现这种不一致？
本文没有给出明确的理论解释，但我们可以从LLM训练数据角度推测：
1. 在训练数据中，很多文本讨论"人类专家经验"vs"算法"的时候，文本表达往往更倾向于人类专家更可靠，所以当直接问信任度的时候，LLM复制了文本中的这种倾向，表现出算法厌恶；
2. 在显示偏好条件，问题给了具体绩效数据，让LLM做选择，训练数据中更多的上下文是"AI算法比人准"，所以LLM倾向于选算法，哪怕数据显示算法更差。
3. 这种不一致本质上是LLM训练数据中不同语境存在矛盾的表达，LLM在不同问题框架下激活了不同的模式，所以表现出不一致的偏见。

### 与现有偏见研究框架的差异
现有LLM偏见研究大多关注静态偏见（比如对特定群体的偏见），而本文发现的是**框架依赖的动态不一致偏见**，偏见方向会因为问题呈现方式变化而反转，这种偏见更隐蔽，也更危险，因为评估者很容易只看一种框架下的结果，得出错误结论。

---

## 5. 实验分析与实证评估
### 使用数据集
本文没有使用传统数据集，而是自己构造了标准化的实验prompt，在8个不同LLM上做实验，属于行为实验研究，这种设计符合研究问题性质。

### 对比 baseline
本文以人类算法厌恶的经典实验结果作为baseline，对比LLM和人类的偏见表现。

### 评估指标
1. 陈述偏好：人类专家和算法Agent平均信任度评分差异；
2. 显示偏好：选择算法Agent和人类专家的比例差异；
3. 跨模型一致性：不同LLM模型结果是否一致。

### 关键实验结果
1. **陈述偏好结果**: 所有8个LLM都给出人类专家更高平均信任评分，差异统计显著，和人类算法厌恶结果一致，相关性很高；
2. **显示偏好结果**: 所有LLM都更高概率选择算法Agent，哪怕算法绩效比人类差，LLM还是更愿意选算法，结果和陈述偏好完全相反；
3. **跨模型一致性**: 无论大小模型，无论开源闭源，结果都一致，说明这是LLM普遍存在的现象，不是特定模型独有；
4. **任务领域一致性**: 在不同任务领域（医疗、金融、气象等）结果都一致，不依赖任务领域。

### 消融实验
本文做了任务领域消融，不同领域结果都一致，说明结论不依赖特定任务，具有普遍性。还做了模型大小消融，小模型和大模型表现一致，说明这种偏见不是因为模型能力不足，而是普遍存在。

### 泛化/鲁棒性分析
结论在8个不同模型、多个不同任务领域都成立，泛化性很好，鲁棒性高。

### 实验公平性分析
- **对比公平性**: 控制了所有变量，只改变任务呈现方式，对比很公平；
- **缺少的baseline**: 可以增加"对人不对事"的控制实验，比如两个都是人类专家，一个标记为算法，看看身份标签本身是否足够导致偏见，本文没有做这个，是一个小缺失；
- **数据泄漏**: 行为实验不存在数据泄漏；
- **统计显著性**: 样本量足够，差异统计显著，结论可靠。

---

## 6. 优势分析（技术层面）
1. **问题抓得准**: 抓住了LLM决策中一个非常重要但之前完全被忽略的问题——来源偏见，问题本身就很有价值；
2. **实验设计清晰**: 借鉴成熟行为经济学范式，设计两种不同任务呈现方式，完美揭示了不一致偏见现象，设计简洁清晰；
3. **结论惊人且重要**: 发现了完全相反的不一致偏见，这个结论对LLM安全评估有非常大的警示作用，提醒大家评估鲁棒性的重要性；
4. **结论普遍性强**: 在8个不同LLM上都验证了结论，说明这是普遍现象，不是个案；
5. **现实意义大**: 对高风险场景下LLM决策部署有直接指导意义，需要我们重视这种框架依赖的不一致偏见。

---

## 7. 局限性与问题
### 理论限制
1. **只发现现象，没有解释原因**: 本文通过实验发现了不一致偏见现象，但没有深入解释为什么会出现这种现象，也没有提出理论模型预测这种情况什么时候发生；
2. **没有研究解决方法**: 本文只揭示了问题，没有研究如何减轻或者消除这种不一致偏见，留下了后续研究空间。

### 可扩展性
实验方法本身可以很容易扩展到更多模型、更多任务场景，可扩展性很好。

### 计算成本
本文是行为实验，计算成本很低，只需要调用LLM API跑实验即可，成本不高。

### 数据依赖
不依赖特定数据集，只需要构造实验prompt，数据依赖很低。

### 可复现性
论文给出了详细实验设计和prompt，很容易复现，可复现性好。

### 工程落地难度
工程落地难度低，只需要在评估LLM决策系统的时候，增加不同框架下的偏见测试就可以检测这个问题，落地门槛低。

---

## 8. 与现有LLM研究的联系
### 与Agent LLM研究的关联
现在LLM agent非常热门，很多系统都是LLM作为控制中枢，调用多个工具（算法Agent）和人类专家输入，整合信息做决策。本文发现告诉我们，LLM本身对不同来源信息就存在不一致偏见，哪怕工具输出更准，在某些框架下LLM也可能选错，这提醒LLM agent设计者要重视这个问题，需要对来源做去偏处理，不能完全让LLM自由权衡。

### 与AI对齐和安全研究的关联
本文属于AI安全和伦理研究，拓展了LLM偏见研究的范围，之前大家关注社会偏见，现在发现还有这种来源框架依赖的不一致偏见，这种偏见在高风险决策场景下可能带来严重危害，拓展了对齐研究的问题边界。

### 与LLM评估研究的关联
本文对LLM评估领域有重要警示：LLM的偏见表达高度依赖任务呈现框架，只在一种框架下评估可能得出完全错误的结论，评估必须考虑多种不同框架，测试评估鲁棒性，这个结论对未来LLM评估方法改进有重要指导意义。

### 与工具调用研究的关联
LLM工具调用研究关注LLM能否正确调用工具，能不能正确解析工具输出，但很少关注LLM对工具本身是否存在偏见，本文揭示了LLM对算法工具可能存在不一致偏见，哪怕工具输出正确，偏见也可能导致LLM做出错误选择，所以工具调用系统需要考虑去偏设计。

---

## 9. 深度批判性思考（最重要）
### "真正创新"是否成立？
我认为真正创新完全成立，这是第一个发现LLM对算法Agent和人类专家存在不一致框架依赖偏见的研究，问题新颖，结论重要，创新程度很高。

### 是否只是 engineering stacking？
完全不是，这是一篇基础实验研究，提出了全新的问题，发现了重要的新现象，不是简单工程堆砌。

### 如果模型扩大 10x 是否仍成立？
我认为结论仍然成立，因为这种偏见本质来源于训练数据中不同语境的矛盾表达，和模型大小关系不大，本文实验已经验证从小模型到大模型结论一致，所以参数扩大10x结论仍然成立。

### 有哪些关键实验缺失？
1. **身份标签控制实验**: 缺少控制实验验证，到底是身份标签"算法Agent"vs"人类专家"本身导致偏见，还是有其他因素。比如做控制实验，同一个专家，一半次数标记为算法，一半标记为人类，看看是否仅仅标签就会导致偏见，我认为这个实验很有必要，本文没做；
2. **真实决策场景扩展**: 本文用的是标准化实验prompt，在真实复杂决策场景中，这种不一致偏见是否仍然存在，还需要进一步验证；
3. **不同偏见程度量化**: 没有量化不同模型偏见程度大小和模型哪些属性相关，比如训练数据中多少比例讨论"AI比人准"，多少讨论"人比AI准"，是否和偏见程度相关，还需要进一步研究。

### 是否存在隐含但未讨论的假设？
本文隐含一个假设：LLM的外显回答能够反映其真实偏好，这个假设在大多数情况下是成立的，但我们知道LLM有时候会出现吐词错误，不过程度不影响结论。还有一个隐含假设：实验设计中的"算法Agent"和"人类专家"框架被LLM正确理解，这个也基本成立。

### 未来可以如何改进？
1. **研究偏见成因**: 进一步研究为什么会出现这种不一致偏见，从训练数据、模型结构角度解释成因，建立预测理论；
2. **研究去偏方法**: 探索如何通过prompt工程、微调、系统设计等方法减轻这种不一致偏见，给出实用解决方案；
3. **扩展到更多场景**: 在真实复杂LLM agent决策系统中验证这个结论，看看实际系统中影响到底有多大；
4. **研究其他来源偏见**: 除了人类vs算法，还有其他来源偏见，比如不同机构输出，不同品牌模型，是否也存在类似框架依赖不一致偏见，可以扩展研究。

### 个人总结与洞察
这篇论文非常棒，它发现了一个非常反直觉但又非常重要的现象：LLM对算法Agent和人类专家的偏见是**框架依赖的**，在直接问信任的时候和人类一样讨厌算法，但是让它做选择下注的时候，反而又盲目相信算法，哪怕算法表现更差。这种不一致性非常危险，因为它完全依赖问题怎么问，评估者很容易被单一框架的评估结果误导。

这个发现对当前火热的LLM agent开发特别有启示，很多人都在做"LLM+工具+人类反馈"的混合决策系统，默认LLM能够合理整合不同来源信息，但本文告诉我们，LLM本身就带着不一致的偏见，设计系统的时候必须要考虑这个问题，不能完全放手让LLM去权衡。比如，我们不能直接问LLM"你更信任哪个"，也不能只给信息让LLM选，必须要设计去偏机制，比如强制要求LLM必须严格根据绩效数据权衡，而不是被来源身份标签影响。

另外，这个发现也给LLM评估敲了警钟：评估LLM不能只用一种方式提问，必须用多种不同框架测试，否则很容易得到错误结论，以为模型没问题，实际上换个问法结论完全反过来。未来LLM评估必须要增加评估鲁棒性测试，检查偏见是否框架依赖。

总的来说，这是一篇小而美的研究，问题抓得准，实验做得清楚，结论惊人且重要，对LLM安全落地有很大指导意义。

## 参考文献
- [论文原文链接](https://arxiv.org/abs/2602.22070)
